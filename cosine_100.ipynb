{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x) : \n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "'''\n",
    " In ADVI, the variational distribution is taken as gaussian with diagonal variance. p(x) ~ N(mu,std^2)\n",
    "'''\n",
    "def variational_pdf(x, mu, std, b, a) : \n",
    "    return stats.norm.pdf(x, mu, std) / ((b - a) * sigmoid(x) * (1 - sigmoid(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.loadtxt('./data/crystal2.txt',delimiter=',')\n",
    "data2 = np.loadtxt('./data/crystal3.txt',delimiter=',')\n",
    "data3 = np.loadtxt('./data/crystal4.txt',delimiter=',')\n",
    "data4 = np.loadtxt('./data/crystal6.txt',delimiter=',')\n",
    "data5 = np.loadtxt('./data/crystal7.txt',delimiter=',')\n",
    "\n",
    "data = np.hstack((data1,data2,data3,data4,data5))\n",
    "\n",
    "sigma1=np.array([ np.sqrt( (data1[3,i]**2)) for i in range(len(data1[0])) ])\n",
    "sigma2=np.array([ np.sqrt( (data2[3,i]**2)) for i in range(len(data2[0])) ])\n",
    "sigma3=np.array([ np.sqrt( (data3[3,i]**2)) for i in range(len(data3[0])) ])\n",
    "sigma4=np.array([ np.sqrt( (data4[3,i]**2)) for i in range(len(data4[0])) ])\n",
    "sigma5=np.array([ np.sqrt( (data5[3,i]**2)) for i in range(len(data5[0])) ])\n",
    "\n",
    "sigma = np.hstack((sigma1,sigma2,sigma3,sigma4,sigma5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_func(C,p0,p1,A,w,t0,x):\n",
    "    return C + p0 * np.exp(-np.log(2) * x / p1) + A * np.cos(w * (x - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hyp_1 : \n",
    "    C1 = pm.Uniform('C1',lower=0.,upper=400.)\n",
    "    C2 = pm.Uniform('C2',lower=0.,upper=400.)\n",
    "    C3 = pm.Uniform('C3',lower=0.,upper=400.)\n",
    "    C4 = pm.Uniform('C4',lower=0.,upper=400.)\n",
    "    C5 = pm.Uniform('C5',lower=0.,upper=400.)\n",
    "    p0_1 = pm.Uniform('p0_1',lower=0.,upper=400.)\n",
    "    p0_2 = pm.Uniform('p0_2',lower=0.,upper=400.)\n",
    "    p0_3 = pm.Uniform('p0_3',lower=0.,upper=400.)\n",
    "    p0_4 = pm.Uniform('p0_4',lower=0.,upper=400.)\n",
    "    p0_5 = pm.Uniform('p0_5',lower=0.,upper=400.)\n",
    "    p1_1 = pm.Uniform('p1_1',lower=0.,upper=30000.)\n",
    "    p1_2 = pm.Uniform('p1_2',lower=0.,upper=30000.)\n",
    "    p1_3 = pm.Uniform('p1_3',lower=0.,upper=30000.)\n",
    "    p1_4 = pm.Uniform('p1_4',lower=0.,upper=30000.)\n",
    "    p1_5 = pm.Uniform('p1_5',lower=0.,upper=30000.)\n",
    "    A = pm.Uniform('A',lower=0.,upper=400.)\n",
    "    t0 = pm.Uniform('t0',lower=0.,upper=360.)\n",
    "    w = pm.Uniform('w',lower=0.0104,upper=0.428)\n",
    "    mean1 = cosine_func(C1,p0_1,p1_1,A,w,t0,data1[0])\n",
    "    mean2 = cosine_func(C2,p0_2,p1_2,A,w,t0,data2[0])\n",
    "    mean3 = cosine_func(C3,p0_3,p1_3,A,w,t0,data3[0])\n",
    "    mean4 = cosine_func(C4,p0_4,p1_4,A,w,t0,data4[0])\n",
    "    mean5 = cosine_func(C5,p0_5,p1_5,A,w,t0,data5[0])\n",
    "    mean = pm.math.concatenate([mean1,mean2,mean3,mean4,mean5])\n",
    "    pm.Normal('y',mu=mean,sd=sigma,observed=data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hyp_1:\n",
    "    advi = pm.ADVI(random_state=1234)\n",
    "tracker = pm.callbacks.Tracker(\n",
    "    mean=advi.approx.mean.eval,  # callable that returns mean\n",
    "    std=advi.approx.std.eval  # callable that returns std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = -114.21:  21%|██▏       | 321599/1500000 [01:40<06:06, 3211.94it/s]   \n",
      "Convergence achieved at 321600\n",
      "Interrupted at 321,599 [21%]: Average Loss = 2.2035e+08\n"
     ]
    }
   ],
   "source": [
    "approx = advi.fit(1500000,obj_optimizer=pm.adam(learning_rate=5e-5),callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute'), tracker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = approx.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = approx.bij.rmap(approx.params[0].eval())\n",
    "rho_dict = approx.bij.rmap(approx.params[1].eval())\n",
    "std_dict ={k: np.log(1 + np.exp(v)) for k, v in rho_dict.items()}\n",
    "\n",
    "# print(means_dict)\n",
    "# print(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp1_logp(trace):\n",
    "    log_p = 0.\n",
    "    for var in ['C1','C2','C3','C4','C5','p0_1','p0_2','p0_3','p0_4','p0_5','A']:\n",
    "        log_p += stats.uniform.logpdf(trace[var],0.,400.)\n",
    "    for var in ['p1_1','p1_2','p1_3','p1_4','p1_5']:\n",
    "        log_p += stats.uniform.logpdf(trace[var],0.,30000.)\n",
    "    log_p += stats.uniform.logpdf(trace['t0'],0.,360.)\n",
    "    log_p += stats.uniform.logpdf(trace['w'],0.0104,0.428-0.0104)\n",
    "    \n",
    "    mean1 = cosine_func(trace['C1'],trace['p0_1'],trace['p1_1'],trace['A'],trace['w'],trace['t0'],data1[0])\n",
    "    mean2 = cosine_func(trace['C2'],trace['p0_2'],trace['p1_2'],trace['A'],trace['w'],trace['t0'],data2[0])\n",
    "    mean3 = cosine_func(trace['C3'],trace['p0_3'],trace['p1_3'],trace['A'],trace['w'],trace['t0'],data3[0])\n",
    "    mean4 = cosine_func(trace['C4'],trace['p0_4'],trace['p1_4'],trace['A'],trace['w'],trace['t0'],data4[0])\n",
    "    mean5 = cosine_func(trace['C5'],trace['p0_5'],trace['p1_5'],trace['A'],trace['w'],trace['t0'],data5[0])\n",
    "    mean =  np.hstack([mean1,mean2,mean3,mean4,mean5])\n",
    "    #log_p += pm.Normal.dist(mu=mean,sd=sigma).logp(data[1]).eval().sum()\n",
    "    log_p += stats.norm.logpdf(data[1],mean,sigma).sum()\n",
    "    return log_p\n",
    "\n",
    "hyp1_logp_vec = np.vectorize(hyp1_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(trace,model):\n",
    "    qw=1.\n",
    "    l=0.\n",
    "    u=400.\n",
    "    for var in model.vars:\n",
    "        var = str(var).split('~')[0].strip()\n",
    "        if(str(var)[:len(str(var)) - len(\"_interval__\")]) in ['p1_1','p1_2','p1_3','p1_4','p1_5']:\n",
    "            l =0.\n",
    "            u = 30000.\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 'w':\n",
    "            l = 0.0104\n",
    "            u = 0.428\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 't0':\n",
    "            l = 0.\n",
    "            u = 360.\n",
    "        else :\n",
    "            l = 0.\n",
    "            u = 400.\n",
    "        qw *= variational_pdf(trace[str(var)],means_dict[str(var)], std_dict[str(var)],u,l)\n",
    "        #print(qw)\n",
    "    return qw\n",
    "\n",
    "q_vec = np.vectorize(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx evidence :  3.229610974421338e+53\n",
      "log approx evidence :  123.20937161730973\n"
     ]
    }
   ],
   "source": [
    "# approximate evidence\n",
    "approx_evidence_1 = (np.exp(hyp1_logp_vec(trace)) / q_vec(trace,hyp_1)).mean()\n",
    "print('Approx evidence : ', approx_evidence_1)\n",
    "print('log approx evidence : ', np.log(approx_evidence_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114.05411289258159\n"
     ]
    }
   ],
   "source": [
    "#ELBO\n",
    "elbo_1 = (hyp1_logp_vec(trace) - np.log(q_vec(trace,hyp_1))).mean()\n",
    "print(elbo_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hyp_2 : \n",
    "    C1 = pm.Uniform('C1',lower=0.,upper=400.)\n",
    "    C2 = pm.Uniform('C2',lower=0.,upper=400.)\n",
    "    C3 = pm.Uniform('C3',lower=0.,upper=400.)\n",
    "    C4 = pm.Uniform('C4',lower=0.,upper=400.)\n",
    "    C5 = pm.Uniform('C5',lower=0.,upper=400.)\n",
    "    p0_1 = pm.Uniform('p0_1',lower=0.,upper=400.)\n",
    "    p0_2 = pm.Uniform('p0_2',lower=0.,upper=400.)\n",
    "    p0_3 = pm.Uniform('p0_3',lower=0.,upper=400.)\n",
    "    p0_4 = pm.Uniform('p0_4',lower=0.,upper=400.)\n",
    "    p0_5 = pm.Uniform('p0_5',lower=0.,upper=400.)\n",
    "    p1_1 = pm.Uniform('p1_1',lower=0.,upper=30000.)\n",
    "    p1_2 = pm.Uniform('p1_2',lower=0.,upper=30000.)\n",
    "    p1_3 = pm.Uniform('p1_3',lower=0.,upper=30000.)\n",
    "    p1_4 = pm.Uniform('p1_4',lower=0.,upper=30000.)\n",
    "    p1_5 = pm.Uniform('p1_5',lower=0.,upper=30000.)\n",
    "    \n",
    "    mean1 = cosine_func(C1,p0_1,p1_1,0,0,0,data1[0])\n",
    "    mean2 = cosine_func(C2,p0_2,p1_2,0,0,0,data2[0])\n",
    "    mean3 = cosine_func(C3,p0_3,p1_3,0,0,0,data3[0])\n",
    "    mean4 = cosine_func(C4,p0_4,p1_4,0,0,0,data4[0])\n",
    "    mean5 = cosine_func(C5,p0_5,p1_5,0,0,0,data5[0])\n",
    "    mean = pm.math.concatenate([mean1,mean2,mean3,mean4,mean5])\n",
    "    pm.Normal('y',mu=mean,sd=sigma,observed=data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hyp_2:\n",
    "    advi_2 = pm.ADVI(random_state=1234)\n",
    "tracker_2 = pm.callbacks.Tracker(\n",
    "    mean=advi_2.approx.mean.eval,  # callable that returns mean\n",
    "    std=advi_2.approx.std.eval  # callable that returns std\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = -126.7:  22%|██▏       | 328599/1500000 [01:28<05:14, 3721.51it/s]    \n",
      "Convergence achieved at 328600\n",
      "Interrupted at 328,599 [21%]: Average Loss = 1.8856e+08\n"
     ]
    }
   ],
   "source": [
    "approx_2 = advi_2.fit(1500000,obj_optimizer=pm.adam(learning_rate=5e-5),callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute'), tracker_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_2 = approx_2.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict = approx_2.bij.rmap(approx_2.params[0].eval())\n",
    "rho_dict = approx_2.bij.rmap(approx_2.params[1].eval())\n",
    "std_dict ={k: np.log(1 + np.exp(v)) for k, v in rho_dict.items()}\n",
    "\n",
    "# print(means_dict)\n",
    "# print(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp2_logp(trace):\n",
    "    log_p = 0.\n",
    "    for var in ['C1','C2','C3','C4','C5','p0_1','p0_2','p0_3','p0_4','p0_5']:\n",
    "        log_p += stats.uniform.logpdf(trace[var],0.,400.)\n",
    "    for var in ['p1_1','p1_2','p1_3','p1_4','p1_5']:\n",
    "        log_p += stats.uniform.logpdf(trace[var],0.,30000.)\n",
    "\n",
    "    \n",
    "    mean1 = cosine_func(trace['C1'],trace['p0_1'],trace['p1_1'],0,0,0,data1[0])\n",
    "    mean2 = cosine_func(trace['C2'],trace['p0_2'],trace['p1_2'],0,0,0,data2[0])\n",
    "    mean3 = cosine_func(trace['C3'],trace['p0_3'],trace['p1_3'],0,0,0,data3[0])\n",
    "    mean4 = cosine_func(trace['C4'],trace['p0_4'],trace['p1_4'],0,0,0,data4[0])\n",
    "    mean5 = cosine_func(trace['C5'],trace['p0_5'],trace['p1_5'],0,0,0,data5[0])\n",
    "    mean =  np.hstack([mean1,mean2,mean3,mean4,mean5])\n",
    "    log_p += stats.norm.logpdf(data[1],mean,sigma).sum()\n",
    "    return log_p\n",
    "\n",
    "hyp2_logp_vec = np.vectorize(hyp2_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx evidence :  9.029215782403812e+57\n",
      "log approx evidence :  133.44781581851572\n"
     ]
    }
   ],
   "source": [
    "# approximate evidence\n",
    "approx_evidence_2 = (np.exp(hyp2_logp_vec(trace_2)) / q_vec(trace_2,hyp_2)).mean()\n",
    "print('Approx evidence : ', approx_evidence_2)\n",
    "print('log approx evidence : ', np.log(approx_evidence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.98068307517603\n"
     ]
    }
   ],
   "source": [
    "#ELBO\n",
    "elbo_2 = (hyp2_logp_vec(trace_2) - np.log(q_vec(trace_2,hyp_2))).mean()\n",
    "print(elbo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.576845489411409e-05"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_evidence_1/approx_evidence_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = (approx_evidence_2/approx_evidence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27957.595679218335\n"
     ]
    }
   ],
   "source": [
    "print(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.238444201206004"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.446499819858114"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(bf)/np.log(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nestle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nestle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bg(x,c,p0,p1):\n",
    "    return c + p0*np.exp(-np.log(2)*x/p1)\n",
    "\n",
    "\n",
    "def log_likelihood_bg(P):\n",
    "    \n",
    "    sigma1=[ data1[3,i] for i in range(len(data1[0])) ]\n",
    "    y_fit1=fit_bg(data1[0],P[0],P[1],P[2])\n",
    "\n",
    "    sigma2=[  data2[3,i]  for i in range(len(data2[0])) ]\n",
    "    y_fit2=fit_bg(data2[0],P[3],P[4],P[5])\n",
    "\n",
    "    sigma3=[ data3[3,i]  for i in range(len(data3[0])) ]\n",
    "    y_fit3=fit_bg(data3[0],P[6],P[7],P[8])\n",
    "\n",
    "    sigma4=[  data4[3,i]  for i in range(len(data4[0])) ]\n",
    "    y_fit4=fit_bg(data4[0],P[9],P[10],P[11])\n",
    "\n",
    "    sigma5=[ data5[3,i]  for i in range(len(data5[0])) ]\n",
    "    y_fit5=fit_bg(data5[0],P[12],P[13],P[14])    \n",
    "\n",
    "    y_fit=np.hstack((y_fit1,y_fit2,y_fit3,y_fit4,y_fit5))\n",
    "    sigma=np.hstack((sigma1,sigma2,sigma3,sigma4,sigma5))\n",
    "\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(data[1],y_fit,sigma))\n",
    "\n",
    "\n",
    "a1=100.0*np.max(data[1])\n",
    "b1=30000.0\n",
    "\n",
    "\n",
    "def prior_transform_bgnoprior(P):\n",
    "    return np.array([a1*P[0],a1*P[1],b1*P[2],a1*P[3],a1*P[4],P[5]*b1,a1*P[6],P[7]*a1,P[8]*b1,P[9]*a1,P[10]*a1,P[11]*b1,P[12]*a1,P[13]*a1,P[14]*b1])\n",
    "\n",
    "result_1 = nestle.sample(log_likelihood_bg, prior_transform_bgnoprior, 15)\n",
    "\n",
    "print(result_1.logz)     # log evidence\n",
    "print(result_1.logzerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cosine(x,c,p0,p1,A,w,t_0):\n",
    "    return c + p0*np.exp(-np.log(2)*x/p1) + A*np.cos(w*(x-t_0))\n",
    "\n",
    "\n",
    "def log_likelihood_cosine(P):\n",
    "    A = P[15]\n",
    "    w = P[16]\n",
    "    t_0=P[17]\n",
    "    sigma1=[ np.sqrt( (data1[3,i]**2)) for i in range(len(data1[0])) ]\n",
    "    y_fit1=fit_cosine(data1[0],P[0],P[1],P[2],P[15],P[16],P[17])\n",
    "    \n",
    "    sigma2=[ np.sqrt( (data2[3,i]**2)) for i in range(len(data2[0])) ]    \n",
    "    y_fit2=fit_cosine(data2[0],P[3],P[4],P[5],P[15],P[16],P[17])\n",
    "    \n",
    "    sigma3=[ np.sqrt( (data3[3,i]**2)) for i in range(len(data3[0])) ]\n",
    "    y_fit3=fit_cosine(data3[0],P[6],P[7],P[8],P[15],P[16],P[17])\n",
    "    \n",
    "    sigma4=[ np.sqrt( (data4[3,i]**2)) for i in range(len(data4[0])) ]\n",
    "    y_fit4=fit_cosine(data4[0],P[9],P[10],P[11],P[15],P[16],P[17])\n",
    "    \n",
    "    sigma5=[ np.sqrt( (data5[3,i]**2) ) for i in range(len(data5[0])) ]\n",
    "    y_fit5=fit_cosine(data5[0],P[12],P[13],P[14],P[15],P[16],P[17])\n",
    "    \n",
    "    sigma=np.hstack((sigma1,sigma2,sigma3,sigma4,sigma5))            \n",
    "    y_fit=np.hstack((y_fit1,y_fit2,y_fit3,y_fit4,y_fit5))\n",
    "    \n",
    "    sigma=np.hstack((sigma1,sigma2,sigma3,sigma4,sigma5))            \n",
    "    y_fit=np.hstack((y_fit1,y_fit2,y_fit3,y_fit4,y_fit5))\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(data[1],y_fit,sigma))\n",
    "\n",
    "\n",
    "a1=100.0*np.max(data[1])\n",
    "b1=30000.0\n",
    "\n",
    "def prior_transform_cos(P):\n",
    "        return np.array([a1*P[0],a1*P[1],b1*P[2],a1*P[3],a1*P[4],P[5]*b1,a1*P[6],P[7]*a1,P[8]*b1,P[9]*a1,P[10]*a1,P[11]*b1,P[12]*a1,P[13]*a1,P[14]*b1,P[15]*a1,P[16]*0.40759+0.0104,P[17]*361.0])\n",
    "\n",
    "result_2 = nestle.sample(log_likelihood_cosine, prior_transform_cos, 18)\n",
    "\n",
    "print(result_2.logz)     # log evidence\n",
    "print(result_2.logzerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
