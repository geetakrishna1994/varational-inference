{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import emcee\n",
    "import corner\n",
    "import pymc3 as pm\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for this project (Obtained from Pitkin (https://github.com/mattpitkin/periodicG))\n",
    "Ggrad = (6.676e-11-6.672e-11)/(372.31-96.28)\n",
    "UWup = 6.672e-11 + Ggrad*(249.17-96.28)\n",
    "UWuperr = (316.94-249.17)*Ggrad\n",
    "G = np.array([6.67248e-11, 6.67398e-11, 6.67228e-11, 6.674255e-11, 6.67559e-11, UWup, 6.67387e-11, 6.67234e-11, 6.674252e-11, 6.67554e-11, 6.67349e-11, 6.67191e-11])\n",
    "Err = np.array([0.00043e-11, 0.00070e-11, 0.00087e-11, 0.000092e-11, 0.00027e-11, UWuperr, 0.00027e-11, 0.00014e-11, 0.000120e-11, 0.00016e-11, 0.00018e-11, 0.00099e-11])\n",
    "Year = np.array([1981.90,1996.97,1998.32,2000.46,2001.16,2002.02,2003.39,2004.40,2006.48,2007.68,2009.17,2013.57])\n",
    "relYear = Year - Year[0]\n",
    "\n",
    "posgrad = (2015.-1981.) / (674.04-92.79)\n",
    "positions = np.array([108.17, 365.87, 388.94, 425.48, 437.50, 452.08, 475.48, 492.79, 528.44, 552.61, 574.52, 649.52])\n",
    "positions = positions-92.79\n",
    "years = positions*posgrad\n",
    "t_0 = (420.83-92.79)*posgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeserrs = (3./12.)*np.ones_like(Err) # three month errors\n",
    "timeserrs[7] = (1./52.) # one week error on JILA-10\n",
    "timeserrs[11] = (1./52.) # one week error on LENS-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of parameters\n",
    "meanG = np.mean(G)\n",
    "sigma_meanG = np.std(G) / np.sqrt(len(G))  \n",
    "muGmin = meanG - 6.*sigma_meanG\n",
    "muGmax = meanG + 6.*sigma_meanG\n",
    "\n",
    "sigmasysmax = np.max(G)-np.min(G)\n",
    "sigmasysmin = np.min(Err)\n",
    "\n",
    "Amin = sigmasysmin\n",
    "Amax = sigmasysmax\n",
    "\n",
    "timediff = np.diff(np.sort(years)[1:]) # skip the first longer time\n",
    "\n",
    "periodmin = 5.90\n",
    "periodmax = 5.90\n",
    "\n",
    "phimin = 0\n",
    "phimax = 0\n",
    "\n",
    "priorSet = [muGmin,muGmax,sigmasysmin,sigmasysmax,Amin,Amax,periodmin,periodmax,phimin,phimax]\n",
    "data = [G,Err,years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x) : \n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "'''\n",
    " In ADVI, the variational distribution is taken as gaussian with diagonal variance. p(x) ~ N(mu,std^2)\n",
    "'''\n",
    "def variational_pdf(x, mu, std, b, a) : \n",
    "    return (stats.norm.pdf(x, mu, std) / ((b - a) * sigmoid(x) * (1 - sigmoid(x)))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyp 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = -232.14:  93%|█████████▎| 92599/100000 [00:13<00:01, 6912.19it/s]\n",
      "Convergence achieved at 92600\n",
      "Interrupted at 92,599 [92%]: Average Loss = -223.81\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as hyp_1:\n",
    "    muG = pm.Uniform('muG', lower=muGmin, upper=muGmax)\n",
    "    y,sigma_y,time = data\n",
    "    mu = np.repeat(muG, len(y))\n",
    "    y_obs = pm.Normal('y_obs',mu=mu,sd=sigma_y,observed=y)\n",
    "\n",
    "with hyp_1:\n",
    "    approx1 = pm.fit(n=100000,method='advi',obj_optimizer=pm.adam(learning_rate=1e-4),callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute',tolerance=1e-4)])\n",
    "    trace1 = pm.sample_approx(approx1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean :  [0.35940277]\n",
      "std :  [0.05447173]\n"
     ]
    }
   ],
   "source": [
    "# get variational parameters in unconstrained space\n",
    "mu = approx1.mean.eval()\n",
    "std = approx1.std.eval()\n",
    "\n",
    "print('mean : ', mu)\n",
    "print('std : ', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_1_logp(muG):\n",
    "    log_p = 0.\n",
    "    log_p += stats.uniform.logpdf(muG,muGmin,muGmax - muGmin)\n",
    "    mu = np.repeat(muG, len(y))\n",
    "    log_p += stats.norm.logpdf(y,mu,sigma_y).sum()\n",
    "    return log_p\n",
    "\n",
    "hyp_1_logp_vec = np.vectorize(hyp_1_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx evidence :  6.53969331876992e+98\n",
      "log approx evidence :  227.53122938463162\n"
     ]
    }
   ],
   "source": [
    "approx_evidence = (np.exp(hyp_1_logp_vec(trace1['muG'])) / variational_pdf(trace1['muG_interval__'],mu,std,muGmax,muGmin)).mean()\n",
    "print('Approx evidence : ', approx_evidence)\n",
    "print('log approx evidence : ', np.log(approx_evidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.34375660626367\n"
     ]
    }
   ],
   "source": [
    "elbo1 = (hyp_1_logp_vec(trace1['muG']) - np.log(variational_pdf(trace1['muG_interval__'],mu,std,muGmax,muGmin))).mean()\n",
    "print(elbo1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyp 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = -364.67: 100%|██████████| 400000/400000 [01:03<00:00, 6268.92it/s]\n",
      "Finished [100%]: Average Loss = -364.67\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as hyp_2:\n",
    "    muG = pm.Uniform('muG', lower=muGmin, upper=muGmax)\n",
    "    sigmasys = pm.Uniform('sigmasys',lower=sigmasysmin,upper=sigmasysmax)\n",
    "    mu = np.repeat(muG, len(y))\n",
    "    y,sigma_y,time = data\n",
    "    sd = np.sqrt(sigma_y**2 + sigmasys**2)\n",
    "    y_obs = pm.Normal('y_obs',mu=mu,sd=sd,observed=y)\n",
    "with hyp_2:\n",
    "    approx2 = pm.fit(n=400000,method='advi',obj_optimizer=pm.adam(learning_rate=1e-4),callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute',tolerance=1e-4)])\n",
    "    trace2 = pm.sample_approx(approx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sigmasys_interval__': array(-0.73748697), 'muG_interval__': array(0.1038305)}\n",
      "{'sigmasys_interval__': 0.3752807466229987, 'muG_interval__': 0.3751976887136172}\n"
     ]
    }
   ],
   "source": [
    "means_dict = approx2.bij.rmap(approx2.params[0].eval())\n",
    "rho_dict = approx2.bij.rmap(approx2.params[1].eval())\n",
    "std_dict ={k: np.log(1 + np.exp(v)) for k, v in rho_dict.items()}\n",
    "\n",
    "print(means_dict)\n",
    "print(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_2_logp(trace):\n",
    "    log_p = 0.\n",
    "    log_p += stats.uniform.logpdf(trace['muG'],muGmin,muGmax - muGmin)\n",
    "    log_p += stats.uniform.logpdf(trace['sigmasys'],sigmasysmin,sigmasysmax - sigmasysmin)\n",
    "    mu = np.repeat(trace['muG'], len(y))\n",
    "    sd = np.sqrt(sigma_y**2 + trace['sigmasys']**2)\n",
    "    log_p += stats.norm.logpdf(y,mu,sd).sum()\n",
    "    return log_p\n",
    "\n",
    "hyp_2_logp_vec = np.vectorize(hyp_2_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(trace,model):\n",
    "    qw=1.\n",
    "    l=muGmin\n",
    "    u=muGmax\n",
    "    for var in model.vars:\n",
    "        var = str(var).split('~')[0].strip()\n",
    "        #print(str(var))\n",
    "        if(str(var)[:len(str(var)) - len(\"_interval__\")]) == \"muG\":\n",
    "            l=muGmin\n",
    "            u=muGmax\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 'sigmasys':\n",
    "            l = sigmasysmin\n",
    "            u = sigmasysmax\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 'A':\n",
    "            l = Amin\n",
    "            u = Amax\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 'phi':\n",
    "            l = 0\n",
    "            u = 2*np.pi\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 'ts':\n",
    "            l = years - 2.5*timeserrs\n",
    "            u = years\n",
    "        elif (str(var)[:len(str(var)) - len(\"_interval__\")]) == 'P':\n",
    "            l = periodmin\n",
    "            u = periodmax\n",
    "        qw *= variational_pdf(trace[str(var)],means_dict[str(var)], std_dict[str(var)],u,l)\n",
    "        #print(qw)\n",
    "    return qw\n",
    "\n",
    "q_vec = np.vectorize(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx evidence :  2.3712779876432093e+158\n",
      "log approx evidence :  364.67187373816364\n"
     ]
    }
   ],
   "source": [
    "approx_evidence2 = (np.exp(hyp_2_logp_vec(trace2)) / q_vec(trace2,hyp_2)).mean()\n",
    "print('Approx evidence : ', approx_evidence2)\n",
    "print('log approx evidence : ', np.log(approx_evidence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364.62337857289634\n"
     ]
    }
   ],
   "source": [
    "elbo2 = (hyp_2_logp_vec(trace2) - np.log(q_vec(trace2,hyp_2))).mean()\n",
    "print(elbo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyp 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "timediff = np.diff(np.sort(years)[1:])\n",
    "periodmin = 2*np.min(timediff)\n",
    "periodmax = np.max(timediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = -226.25: 100%|██████████| 400000/400000 [01:07<00:00, 5913.68it/s]\n",
      "Finished [100%]: Average Loss = -226.15\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as hyp_3:\n",
    "    y,sigma_y,time = data\n",
    "    muG = pm.Uniform('muG', lower=muGmin, upper=muGmax)\n",
    "    A = pm.Uniform('A', lower=Amin,upper=Amax)\n",
    "    phi = pm.Uniform('phi',lower=0.,upper=2*np.pi)\n",
    "    #ts = pm.Uniform('ts',lower=time - 2.5*timeserrs,upper=time,shape=len(time))\n",
    "    P = pm.Uniform('P', lower=periodmin,upper=periodmax)\n",
    "    mu = muG + A * np.sin(phi + 2 * np.pi * time / P)\n",
    "    y_obs = pm.Normal('y_obs',mu=mu,sd=sigma_y,observed=y)\n",
    "with hyp_3:\n",
    "    approx3 = pm.fit(n=400000,method='advi',obj_optimizer=pm.adam(learning_rate=1e-4),callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute',tolerance=1e-4)])\n",
    "    trace3 = pm.sample_approx(approx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A_interval__': array(-4.82457626), 'phi_interval__': array(-0.19722192), 'P_interval__': array(-0.01651674), 'muG_interval__': array(0.35800202)}\n",
      "{'A_interval__': 0.8368647355258746, 'phi_interval__': 0.8489322223055673, 'P_interval__': 0.816633799550027, 'muG_interval__': 0.05483018600694995}\n"
     ]
    }
   ],
   "source": [
    "means_dict = approx3.bij.rmap(approx3.params[0].eval())\n",
    "rho_dict = approx3.bij.rmap(approx3.params[1].eval())\n",
    "std_dict ={k: np.log(1 + np.exp(v)) for k, v in rho_dict.items()}\n",
    "\n",
    "print(means_dict)\n",
    "print(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_3_logp(trace):\n",
    "    log_p = 0.\n",
    "    log_p += stats.uniform.logpdf(trace['muG'],muGmin,muGmax - muGmin)\n",
    "    log_p += stats.uniform.logpdf(trace['A'],Amin,Amax - Amin)\n",
    "    log_p += stats.uniform.logpdf(trace['phi'],0,2*np.pi)\n",
    "    log_p += stats.uniform.logpdf(trace['P'],periodmin,periodmax-periodmin)\n",
    "    mu = trace['muG'] + trace['A'] * np.sin(trace['phi'] + 2 * np.pi * time / trace['P'])\n",
    "    log_p += stats.norm.logpdf(y,mu,sigma_y).sum()\n",
    "    return log_p\n",
    "\n",
    "hyp_3_logp_vec = np.vectorize(hyp_3_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5.38772646e+27)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vec(trace3[0],hyp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx evidence :  2.4369911755064877e+110\n",
      "log approx evidence :  254.17512438293372\n"
     ]
    }
   ],
   "source": [
    "approx_evidence3 = (np.exp(hyp_3_logp_vec(trace3)) / q_vec(trace3,hyp_3)).mean()\n",
    "print('Approx evidence : ', approx_evidence3)\n",
    "print('log approx evidence : ', np.log(approx_evidence3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226.98970830521176\n"
     ]
    }
   ],
   "source": [
    "elbo3 = (hyp_3_logp_vec(trace3) - np.log(q_vec(trace3,hyp_3))).mean()\n",
    "print(elbo3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyp 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = -362.7: 100%|██████████| 400000/400000 [01:11<00:00, 5620.95it/s] \n",
      "Finished [100%]: Average Loss = -362.7\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as hyp_4:\n",
    "    y,sigma_y,time = data\n",
    "    muG = pm.Uniform('muG', lower=muGmin, upper=muGmax)\n",
    "    A = pm.Uniform('A', lower=Amin,upper=Amax)\n",
    "    phi = pm.Uniform('phi',lower=0.,upper=2*np.pi)\n",
    "    sigmasys = pm.Uniform('sigmasys',lower=sigmasysmin,upper=sigmasysmax)\n",
    "    P = pm.Uniform('P', lower=periodmin,upper=periodmax)\n",
    "    mu = muG + A * np.sin(phi + 2 * np.pi * time / P)\n",
    "    sd = np.sqrt(sigma_y**2 + sigmasys**2)\n",
    "    y_obs = pm.Normal('y_obs',mu=mu,sd=sd,observed=y)\n",
    "with hyp_4:\n",
    "    approx4 = pm.fit(n=400000,method='advi',obj_optimizer=pm.adam(learning_rate=1e-4),callbacks=[pm.callbacks.CheckParametersConvergence(diff='absolute',tolerance=1e-4)])\n",
    "    trace4 = pm.sample_approx(approx4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sigmasys_interval__': array(-0.61789422), 'muG_interval__': array(0.09918406), 'A_interval__': array(-2.36018853), 'phi_interval__': array(-0.13348387), 'P_interval__': array(0.01864589)}\n",
      "{'sigmasys_interval__': 0.38388456863550247, 'muG_interval__': 0.4033109446084607, 'A_interval__': 0.9587299824571236, 'phi_interval__': 1.4958199656598883, 'P_interval__': 1.494047928430552}\n"
     ]
    }
   ],
   "source": [
    "means_dict = approx4.bij.rmap(approx4.params[0].eval())\n",
    "rho_dict = approx4.bij.rmap(approx4.params[1].eval())\n",
    "std_dict ={k: np.log(1 + np.exp(v)) for k, v in rho_dict.items()}\n",
    "\n",
    "print(means_dict)\n",
    "print(std_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyp_4_logp(trace):\n",
    "    log_p = 0.\n",
    "    log_p += stats.uniform.logpdf(trace['muG'],muGmin,muGmax - muGmin)\n",
    "    log_p += stats.uniform.logpdf(trace['A'],Amin,Amax - Amin)\n",
    "    log_p += stats.uniform.logpdf(trace['phi'],0,2*np.pi)\n",
    "    log_p += stats.uniform.logpdf(trace['P'],periodmin,periodmax-periodmin)\n",
    "    log_p += stats.uniform.logpdf(trace['sigmasys'],sigmasysmin,sigmasysmax - sigmasysmin)\n",
    "    mu = trace['muG'] + trace['A'] * np.sin(trace['phi'] + 2 * np.pi * time / trace['P'])\n",
    "    sd = np.sqrt(sigma_y**2 + trace['sigmasys']**2)\n",
    "    log_p += stats.norm.logpdf(y,mu,sd).sum()\n",
    "    return log_p\n",
    "\n",
    "hyp_4_logp_vec = np.vectorize(hyp_4_logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx evidence :  6.800917123103344e+157\n",
      "log approx evidence :  363.4229170741976\n"
     ]
    }
   ],
   "source": [
    "approx_evidence4 = (np.exp(hyp_4_logp_vec(trace4)) / q_vec(trace4,hyp_4)).mean()\n",
    "print('Approx evidence : ', approx_evidence4)\n",
    "print('log approx evidence : ', np.log(approx_evidence4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362.9354069309452\n"
     ]
    }
   ],
   "source": [
    "elbo4 = (hyp_4_logp_vec(trace4) - np.log(q_vec(trace4,hyp_4))).mean()\n",
    "print(elbo4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nestle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nestle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.10272646913808\n",
      "0.17319730047704412\n",
      "CPU times: user 788 ms, sys: 33 ms, total: 821 ms\n",
      "Wall time: 791 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def loglike_1(P):\n",
    "    mu = np.repeat(P[0], len(y))\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y,mu,sigma_y))\n",
    "\n",
    "def prior_transform_1(P):\n",
    "    return np.array([(muGmax - muGmin)*P[0]+muGmin])\n",
    "\n",
    "result_1 = nestle.sample(loglike_1, prior_transform_1, 1)\n",
    "\n",
    "print(result_1.logz)     # log evidence\n",
    "print(result_1.logzerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364.846311269481\n",
      "0.13742716983384742\n",
      "CPU times: user 790 ms, sys: 4.1 ms, total: 794 ms\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def loglike_2(P):\n",
    "    mu = np.repeat(P[0], len(y))\n",
    "    sd = np.sqrt(sigma_y**2 + P[1]**2)\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y,mu,sd))\n",
    "\n",
    "def prior_transform_2(P):\n",
    "    return np.array([(muGmax - muGmin)*P[0] + muGmin, (sigmasysmax - sigmasysmin) * P[1] + sigmasysmin])\n",
    "\n",
    "result_2 = nestle.sample(loglike_2, prior_transform_2, 2)\n",
    "\n",
    "print(result_2.logz)     # log evidence\n",
    "print(result_2.logzerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.5903768451409\n",
      "0.3447761287045049\n",
      "CPU times: user 13min 4s, sys: 2.67 s, total: 13min 7s\n",
      "Wall time: 13min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def loglike_3(P):\n",
    "    mu = P[0] + P[1] * np.sin(P[2] + 2 * np.pi * time / P[3])\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y,mu,sigma_y))\n",
    "\n",
    "def prior_transform_3(P):\n",
    "    return np.array([(muGmax - muGmin)*P[0] + muGmin,\n",
    "                    (Amax - Amin) * P[1] + Amin,\n",
    "                    2*np.pi * P[2], \n",
    "                    (periodmax - periodmin) * P[3] + periodmin])\n",
    "\n",
    "result_3 = nestle.sample(loglike_3, prior_transform_3, 4)\n",
    "print(result_3.logz)     # log evidence\n",
    "print(result_3.logzerr)  # numerical (sampling) error on logz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363.5889785652872\n",
      "0.18057415227619145\n",
      "CPU times: user 15.5 s, sys: 49.6 ms, total: 15.6 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def loglike_4(P):\n",
    "    mu = P[0] + P[1] * np.sin(P[2] + 2 * np.pi * time / P[3])\n",
    "    sd = np.sqrt(sigma_y**2 + P[4]**2)\n",
    "    return sum(stats.norm.logpdf(*args) for args in zip(y,mu,sd))\n",
    "\n",
    "def prior_transform_4(P):\n",
    "    return np.array([(muGmax - muGmin)*P[0] + muGmin,\n",
    "                    (Amax - Amin) * P[1] + Amin,\n",
    "                    2*np.pi * P[2], \n",
    "                    (periodmax - periodmin) * P[3] + periodmin,\n",
    "                    (sigmasysmax - sigmasysmin) * P[4] + sigmasysmin])\n",
    "\n",
    "result_4 = nestle.sample(loglike_4, prior_transform_4, 5)\n",
    "print(result_4.logz)     # log evidence\n",
    "print(result_4.logzerr)  # numerical (sampling) error on logz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import numpy as np\n",
    "# import dynesty\n",
    "# from dynesty import plotting as dyplot\n",
    "\n",
    "\n",
    "# def loglike_3(P):\n",
    "#     mu = P[0] + P[1] * np.sin(P[2] + 2 * np.pi * time / P[3])\n",
    "#     return sum(stats.norm.logpdf(*args) for args in zip(y,mu,sigma_y))\n",
    "\n",
    "# def prior_transform_3(P):\n",
    "#     return np.array([(muGmax - muGmin)*P[0] + muGmin,\n",
    "#                     (Amax - Amin) * P[1] + Amin,\n",
    "#                     2*np.pi * P[2], \n",
    "#                     (periodmax - periodmin) * P[3] + periodmin])\n",
    "\n",
    "\n",
    "# # Sample from our distribution.\n",
    "# sampler = dynesty.NestedSampler(loglike_3, prior_transform_3, 4,\n",
    "#                                 bound='single', nlive=1000)\n",
    "# sampler.run_nested(dlogz=0.01)\n",
    "# res = sampler.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
